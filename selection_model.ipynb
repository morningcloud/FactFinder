{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import xapian\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy \n",
    "import en_core_web_lg\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "vectorizer = DictVectorizer()\n",
    "nlp_large = en_core_web_lg.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### GET TRAINING DATA ###\n",
      "training set: 145449\n",
      "### GET DEVELOPMENT DATA ###\n",
      "development set: 5001\n",
      "### GET TEST DATA ###\n",
      "test set: 14997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dbpath = \"index_V6_nostop\"\n",
    "\n",
    "\n",
    "print(\"### GET TRAINING DATA ###\")\n",
    "\n",
    "train_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"train.json\")\n",
    "with open(train_path) as json_file:  \n",
    "    train = json.load(json_file)\n",
    "\n",
    "rowid = list(train.keys())\n",
    "claim = []\n",
    "label = []\n",
    "evidence = []\n",
    "for idx in rowid:\n",
    "    claim.append(train[idx]['claim'])\n",
    "    label.append(train[idx]['label'])\n",
    "    evidence.append(train[idx]['evidence'])\n",
    "    \n",
    "len_claim = len(claim)    \n",
    "\n",
    "print(\"training set:\",len_claim)\n",
    "\n",
    "print(\"### GET DEVELOPMENT DATA ###\")\n",
    "\n",
    "dev_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"devset.json\")\n",
    "with open(dev_path) as json_file:  \n",
    "    development = json.load(json_file)\n",
    "    \n",
    "drowid = list(development.keys())\n",
    "dclaim = []\n",
    "dlabel = []\n",
    "devidence = []\n",
    "for idx in drowid:\n",
    "    dclaim.append(development[idx]['claim'])\n",
    "    dlabel.append(development[idx]['label'])\n",
    "    devidence.append(development[idx]['evidence'])\n",
    "\n",
    "len_dclaim= len(dclaim)    \n",
    "print(\"development set:\", len_dclaim)\n",
    "\n",
    "print(\"### GET TEST DATA ###\")\n",
    "\n",
    "test_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"test-unlabelled.json\")\n",
    "with open(test_path) as json_file:  \n",
    "    test = json.load(json_file)\n",
    "    \n",
    "trowid = list(test.keys())\n",
    "tclaim = []\n",
    "for idx in trowid:\n",
    "    tclaim.append(test[idx]['claim'])\n",
    "\n",
    "len_tclaim = len(tclaim)    \n",
    "print(\"test set:\", len_tclaim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'SUPPORTS': 80035, 'REFUTES': 29775, 'NOT ENOUGH INFO': 35639})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CONNECT TO INDEX ###\n"
     ]
    }
   ],
   "source": [
    "print(\"### CONNECT TO INDEX ###\")\n",
    "\n",
    "def getstopper():\n",
    "    stopper = xapian.SimpleStopper()\n",
    "    for s in stopwords.words('english'):\n",
    "        stopper.add(s)\n",
    "    return stopper\n",
    "\n",
    "def get_doc_id(match):\n",
    "    for term in match.document.termlist():\n",
    "        term = term.term.decode(\"utf-8\") \n",
    "        m = re.match(\"Q(.*)\", term)\n",
    "        if m:\n",
    "            return m[1]\n",
    "    return None\n",
    "\n",
    "# Prepare enquiry object\n",
    "\n",
    "# Open the database we're going to search.\n",
    "db = xapian.Database(dbpath)\n",
    "\n",
    "# Set up a QueryParser with a stemmer and suitable prefixes\n",
    "queryparser = xapian.QueryParser()\n",
    "queryparser.set_stemmer(xapian.Stem(\"en\"))\n",
    "queryparser.set_stemming_strategy(queryparser.STEM_SOME)\n",
    "queryparser.set_stopper(getstopper())\n",
    "queryparser.add_prefix('keywords', 'K')\n",
    "\n",
    "# Use an Enquire object on the database to run the query\n",
    "enquire = xapian.Enquire(db)\n",
    "\n",
    "def get_match(query,pagesize):\n",
    "    query = queryparser.parse_query(query)\n",
    "    enquire.set_query(query)\n",
    "    matches = enquire.get_mset(0, pagesize)\n",
    "\n",
    "    query_results = []\n",
    "    #doc_title = []\n",
    "    for match in matches:\n",
    "        result = dict(\n",
    "            found_doc = get_doc_id(match),\n",
    "            rank = match.rank + 1, \n",
    "            percent = match.percent,\n",
    "            weight = match.weight,\n",
    "            docid = match.docid,\n",
    "            text = match.document.get_data()\n",
    "        )\n",
    "        query_results.append(result)\n",
    "        #doc_title.append(get_doc_id(match))\n",
    "    return query_results #, doc_title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### NORMALIZATION ###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"### NORMALIZATION ###\")\n",
    "    \n",
    "def replace_punctuation(sentence):\n",
    "    text = re.sub(r\"(-LRB-|-LSB-|-RSB-|-RRB-|-COLON-|-lrb-|-lsb-|-rsb-|-rrb-|-colon-|``|'')*\", \"\", sentence)\n",
    "    text = re.sub(r\"[`']+\", \" \", text)\n",
    "    text = re.sub(r\"[.,:;]*\", \"\", text)\n",
    "    text = text.replace(\"_\",\" \")\n",
    "    text = text.replace(\"\\\\n\",\"\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def get_entity(document):\n",
    "    doc = nlp_large(document)\n",
    "    combine_entity = ''\n",
    "    for entity in doc.ents:\n",
    "        combine_entity = combine_entity + ' ' + str(entity)\n",
    "    return combine_entity.strip()\n",
    "\n",
    "def remove_stopwords(wordlist):\n",
    "    filtered = [w for w in wordlist if w not in stopwords.words('english')]\n",
    "    return filtered\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma  \n",
    "\n",
    "def normalize_counter(document):  \n",
    "    entities = get_entity(document)\n",
    "    combine = document + ' ' + entities\n",
    "    filtered = nltk.word_tokenize(combine)\n",
    "    for i in range(len(filtered)):\n",
    "         filtered[i] = lemmatize(filtered[i].lower())\n",
    "    word_counter = Counter(filtered)\n",
    "    return word_counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### GET FEATURE LIST ###\n"
     ]
    }
   ],
   "source": [
    "print(\"### GET FEATURE LIST ###\")\n",
    "      \n",
    "      \n",
    "feature_list_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"feature_list_complete.json\")\n",
    "with open(feature_list_path) as json_file:  \n",
    "    feature_list_dict = json.load(json_file)\n",
    "    \n",
    "feature_list = []\n",
    "claim_index = []\n",
    "for key in feature_list_dict:\n",
    "    feature_list.append(feature_list_dict[key])\n",
    "    claim_index.append(int(key))\n",
    "    \n",
    "label_list = []\n",
    "for i in claim_index:\n",
    "    label_list.append(label[i])\n",
    "    \n",
    "weight_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"weight_complete.json\")\n",
    "with open(weight_path) as json_file:  \n",
    "    weight_dict = json.load(json_file)\n",
    "    \n",
    "weight_list = []\n",
    "for i in claim_index:\n",
    "    weight_list.append(weight_dict[str(i)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RANDOM LIST ###\n",
      "### RUN VECTORIZER ###\n"
     ]
    }
   ],
   "source": [
    " print(\"### RANDOM LIST ###\")\n",
    "          \n",
    "    \n",
    "import random\n",
    "\n",
    "combined = list(zip(claim_index, feature_list, label_list, weight_list))\n",
    "random.shuffle(combined)\n",
    "claim_index, feature_list, label_list, weight_list = zip(*combined)\n",
    "\n",
    "training_set = feature_list[:120000]\n",
    "training_label = label_list[:120000]\n",
    "heldout_set = feature_list[120000:]\n",
    "heldout_label = label_list[120000:]\n",
    "\n",
    "print(\"### RUN VECTORIZER ###\")\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "training_data = vectorizer.fit_transform(training_set)\n",
    "heldout_data = vectorizer.transform(heldout_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 5), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"### TRAIN THE MODEL ###\")\n",
    "\n",
    "### MLP Classifier 2 hidden layers 10 nodes & 5 nodes ###\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10, 5), random_state=1)\n",
    "clf.fit(training_data, training_label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585141251755892\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9997062279670975, 'recall': 0.9988259465805694, 'f1-score': 0.9992658934077228, 'support': 3407}, 'REFUTES': {'precision': 0.7189859762675297, 'recall': 0.5087786259541984, 'f1-score': 0.5958873491282968, 'support': 2620}, 'SUPPORTS': {'precision': 0.829142403388036, 'recall': 0.9230882569618388, 'f1-score': 0.8735968765251343, 'support': 6787}, 'accuracy': 0.8585141251755892, 'macro avg': {'precision': 0.8492782025408877, 'recall': 0.8102309431655356, 'f1-score': 0.8229167063537179, 'support': 12814}, 'weighted avg': {'precision': 0.8519690860230552, 'recall': 0.8585141251755892, 'f1-score': 0.8502283248425422, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = clf.predict(heldout_data)\n",
    "\n",
    "print(accuracy_score(heldout_label,predictions))\n",
    "results = classification_report(heldout_label,predictions,  output_dict=True)\n",
    "\n",
    "print(results)\n",
    "\n",
    "with open('result1_10_5.json', 'w') as f:\n",
    "    json.dump(results, f, indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_10_5.joblib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf, 'MLPClassifier_lbfgs_10_5.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 2 hidden layers 10 nodes & 10 nodes ###\n",
    "\n",
    "clf2 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10, 10), random_state=1)\n",
    "\n",
    "clf2.fit(training_data, training_label) \n",
    "\n",
    "predictions2 = clf2.predict(heldout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8597627594818168\n",
      "{'NOT ENOUGH INFO': {'precision': 0.998533724340176, 'recall': 0.9994129732902847, 'f1-score': 0.9989731553469268, 'support': 3407}, 'REFUTES': {'precision': 0.7254366812227074, 'recall': 0.5072519083969466, 'f1-score': 0.5970350404312668, 'support': 2620}, 'SUPPORTS': {'precision': 0.8297675647120972, 'recall': 0.9257403860321203, 'f1-score': 0.875130580123964, 'support': 6787}, 'accuracy': 0.8597627594818168, 'macro avg': {'precision': 0.8512459900916601, 'recall': 0.8108017559064505, 'f1-score': 0.8237129253007192, 'support': 12814}, 'weighted avg': {'precision': 0.8533073954527451, 'recall': 0.8597627594818168, 'f1-score': 0.8511974866160639, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = accuracy_score(heldout_label,predictions2)\n",
    "results2 = classification_report(heldout_label,predictions2,  output_dict=True)\n",
    "\n",
    "print(accuracy2)\n",
    "print(results2)\n",
    "\n",
    "with open('result1_10_10.json', 'w') as f:\n",
    "    json.dump(results2, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_10_10.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf2, 'MLPClassifier_lbfgs_10_10.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 1 hidden layers 100 nodes ###\n",
    "\n",
    "clf3 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "clf3.fit(training_data, training_label) \n",
    "\n",
    "predictions3 = clf3.predict(heldout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8667863274543468\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9974055923897377, 'recall': 0.9979809633689068, 'f1-score': 0.9976931949250288, 'support': 3467}, 'REFUTES': {'precision': 0.7014242115971516, 'recall': 0.5522627152583099, 'f1-score': 0.6179699753529017, 'support': 2497}, 'SUPPORTS': {'precision': 0.8494375931698062, 'recall': 0.915036496350365, 'f1-score': 0.8810176400309228, 'support': 6850}, 'accuracy': 0.8667863274543468, 'macro avg': {'precision': 0.8494224657188986, 'recall': 0.8217600583258605, 'f1-score': 0.8322269367696178, 'support': 12814}, 'weighted avg': {'precision': 0.8606296986410552, 'recall': 0.8667863274543468, 'f1-score': 0.861326999334563, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "accuracy3 = accuracy_score(heldout_label,predictions3)\n",
    "results3 = classification_report(heldout_label,predictions3,  output_dict=True)\n",
    "\n",
    "print(accuracy3)\n",
    "print(results3)\n",
    "\n",
    "with open('result1_100.json', 'w') as f:\n",
    "    json.dump(results3, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_100.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf3, 'MLPClassifier_lbfgs_100.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPClassifier_lbfgs_100 = load('MLPClassifier_lbfgs_100.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8637427813329172\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9988249118683902, 'recall': 0.9979454065159965, 'f1-score': 0.9983849654969902, 'support': 3407}, 'REFUTES': {'precision': 0.71338199513382, 'recall': 0.5595419847328245, 'f1-score': 0.6271657754010697, 'support': 2620}, 'SUPPORTS': {'precision': 0.8432358939496941, 'recall': 0.9138058052158539, 'f1-score': 0.8771036628482534, 'support': 6787}, 'accuracy': 0.8637427813329172, 'macro avg': {'precision': 0.851814266983968, 'recall': 0.8237643988215583, 'f1-score': 0.8342181345821045, 'support': 12814}, 'weighted avg': {'precision': 0.8580536377573581, 'recall': 0.8637427813329172, 'f1-score': 0.8582467979358626, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "predictions3_ = MLPClassifier_lbfgs_100.predict(heldout_data)\n",
    "accuracy3_ = accuracy_score(heldout_label,predictions3_)\n",
    "results3_ = classification_report(heldout_label,predictions3_,  output_dict=True)\n",
    "\n",
    "print(accuracy3_)\n",
    "print(results3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_index_refutes = []\n",
    "for i in claim_index:\n",
    "    if label[i] == 'REFUTES':\n",
    "        claim_index_refutes.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26395"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claim_index_refutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indah/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(training_data, training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOT ENOUGH INFO': 63993, 'SUPPORTS': 63993, 'REFUTES': 63993})\n",
      "191979\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_resampled))\n",
    "print(len(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 1 hidden layers 20 nodes ###\n",
    "\n",
    "clf4 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(20,), random_state=1)\n",
    "\n",
    "clf4.fit(training_data, training_label) \n",
    "\n",
    "predictions4 = clf4.predict(heldout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8597627594818168\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9982409850483729, 'recall': 0.9994129732902847, 'f1-score': 0.9988266353769434, 'support': 3407}, 'REFUTES': {'precision': 0.7205333333333334, 'recall': 0.5156488549618321, 'f1-score': 0.6011123470522803, 'support': 2620}, 'SUPPORTS': {'precision': 0.8316950053134963, 'recall': 0.9224988949462207, 'f1-score': 0.8747467691232974, 'support': 6787}, 'accuracy': 0.8597627594818168, 'macro avg': {'precision': 0.8501564412317341, 'recall': 0.8125202410661125, 'f1-score': 0.824895250517507, 'support': 12814}, 'weighted avg': {'precision': 0.853247882820028, 'recall': 0.8597627594818168, 'f1-score': 0.85178890417091, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "accuracy4 = accuracy_score(heldout_label,predictions4)\n",
    "results4 = classification_report(heldout_label,predictions4,  output_dict=True)\n",
    "\n",
    "print(accuracy4)\n",
    "print(results4)\n",
    "\n",
    "with open('result1_20.json', 'w') as f:\n",
    "    json.dump(results4, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_20.joblib']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf4, 'MLPClassifier_lbfgs_20.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 1 hidden layers 20 nodes ###\n",
    "\n",
    "clf5 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(20,), random_state=1)\n",
    "\n",
    "clf5.fit(X_resampled, y_resampled) \n",
    "\n",
    "predictions5 = clf5.predict(heldout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8273763071640393\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9991186839012925, 'recall': 0.9982389198708541, 'f1-score': 0.9986786081339011, 'support': 3407}, 'REFUTES': {'precision': 0.5674525212835625, 'recall': 0.6614503816793893, 'f1-score': 0.6108565385971096, 'support': 2620}, 'SUPPORTS': {'precision': 0.8602894902454373, 'recall': 0.8056578753499337, 'f1-score': 0.8320779121966065, 'support': 6787}, 'accuracy': 0.8273763071640393, 'macro avg': {'precision': 0.8089535651434309, 'recall': 0.821782392300059, 'f1-score': 0.813871019642539, 'support': 12814}, 'weighted avg': {'precision': 0.837326965202936, 'recall': 0.8273763071640393, 'f1-score': 0.8311421054405334, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "accuracy5 = accuracy_score(heldout_label,predictions5)\n",
    "results5 = classification_report(heldout_label,predictions5,  output_dict=True)\n",
    "\n",
    "print(accuracy5)\n",
    "print(results5)\n",
    "\n",
    "with open('result1_20_bootstrap.json', 'w') as f:\n",
    "    json.dump(results5, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_20_bootstrap.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf5, 'MLPClassifier_lbfgs_20_bootstrap.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 3 hidden layers @ 10 nodes ###\n",
    "\n",
    "clf6 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10,10,10), random_state=1)\n",
    "\n",
    "clf6.fit(training_data, training_label) \n",
    "\n",
    "predictions6 = clf6.predict(heldout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614015920087404\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9959689029657357, 'recall': 0.997692529564465, 'f1-score': 0.9968299711815563, 'support': 3467}, 'REFUTES': {'precision': 0.6842900302114804, 'recall': 0.5442531037244693, 'f1-score': 0.60629043051528, 'support': 2497}, 'SUPPORTS': {'precision': 0.8456832087015635, 'recall': 0.908029197080292, 'f1-score': 0.8757479760647661, 'support': 6850}, 'accuracy': 0.8614015920087404, 'macro avg': {'precision': 0.8419807139595932, 'recall': 0.8166582767897421, 'f1-score': 0.8262894592538674, 'support': 12814}, 'weighted avg': {'precision': 0.8548951437198362, 'recall': 0.8614015920087404, 'f1-score': 0.8560004956396721, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "accuracy6 = accuracy_score(heldout_label,predictions6)\n",
    "results6 = classification_report(heldout_label,predictions6,  output_dict=True)\n",
    "\n",
    "print(accuracy6)\n",
    "print(results6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indah/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1278: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "training_matrix_tfidf = transformer.fit_transform(training_data)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2000)\n",
    "training_matrix_lowrank = svd.fit_transform(training_matrix_tfidf)\n",
    "\n",
    "heldout_data = vectorizer.transform(heldout_set)\n",
    "heldout_matrix_tfidf = transformer.fit_transform(heldout_data)\n",
    "heldout_matrix_lowrank = svd.fit_transform(heldout_matrix_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP Classifier 1 hidden layers 100 nodes on tfidf ###\n",
    "\n",
    "clf3.fit(training_matrix_tfidf, training_label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8708443889495864\n",
      "{'NOT ENOUGH INFO': {'precision': 0.9976838448176027, 'recall': 0.9939428901067205, 'f1-score': 0.9958098540673312, 'support': 3467}, 'REFUTES': {'precision': 0.69124218051832, 'recall': 0.6195434521425711, 'f1-score': 0.6534318901795143, 'support': 2497}, 'SUPPORTS': {'precision': 0.8657680426846391, 'recall': 0.9001459854014598, 'f1-score': 0.8826223876324076, 'support': 6850}, 'accuracy': 0.8708443889495864, 'macro avg': {'precision': 0.8515646893401873, 'recall': 0.8378774425502504, 'f1-score': 0.8439547106264177, 'support': 12814}, 'weighted avg': {'precision': 0.8674506560891722, 'recall': 0.8708443889495864, 'f1-score': 0.8685855743024565, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "predictions7 = clf3.predict(heldout_matrix_tfidf)\n",
    "\n",
    "accuracy7 = accuracy_score(heldout_label,predictions7)\n",
    "results7 = classification_report(heldout_label,predictions7,  output_dict=True)\n",
    "\n",
    "print(accuracy7)\n",
    "print(results7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_100_tfidf.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf3, 'MLPClassifier_lbfgs_100_tfidf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP Classifier 1 hidden layers 100 nodes on tfidf ###\n",
    "\n",
    "clf3.fit(training_matrix_lowrank, training_label) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.444982050881848\n",
      "{'NOT ENOUGH INFO': {'precision': 0.4765450483991065, 'recall': 0.5537929045284107, 'f1-score': 0.512273212379936, 'support': 3467}, 'REFUTES': {'precision': 0.23294723294723294, 'recall': 0.36243492190628757, 'f1-score': 0.28361015355687874, 'support': 2497}, 'SUPPORTS': {'precision': 0.5871428571428572, 'recall': 0.42, 'f1-score': 0.4897021276595745, 'support': 6850}, 'accuracy': 0.444982050881848, 'macro avg': {'precision': 0.43221171282973225, 'recall': 0.4454092754782328, 'f1-score': 0.4285284978654631, 'support': 12814}, 'weighted avg': {'precision': 0.4881988055952485, 'recall': 0.444982050881848, 'f1-score': 0.45564892736232626, 'support': 12814}}\n"
     ]
    }
   ],
   "source": [
    "predictions8 = clf3.predict(heldout_matrix_lowrank)\n",
    "accuracy8 = accuracy_score(heldout_label,predictions8)\n",
    "results8 = classification_report(heldout_label,predictions8,  output_dict=True)\n",
    "\n",
    "print(accuracy8)\n",
    "print(results8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "# bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "# ...                                     token_pattern=r'\\b\\w+\\b', min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test Development on Top 5 Sentences By Cosine Similarity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match2(query,pagesize):\n",
    "    query = queryparser.parse_query(query)\n",
    "    enquire.set_query(query)\n",
    "    matches = enquire.get_mset(0, pagesize)\n",
    "\n",
    "    query_results = []\n",
    "    doc_title = []\n",
    "    for match in matches:\n",
    "        result = dict(\n",
    "            found_doc = get_doc_id(match),\n",
    "            rank = match.rank + 1, \n",
    "            percent = match.percent,\n",
    "            weight = match.weight,\n",
    "            docid = match.docid,\n",
    "            text = match.document.get_data()\n",
    "        )\n",
    "        query_results.append(result)\n",
    "        doc_title.append(get_doc_id(match))\n",
    "    return query_results , doc_title\n",
    "\n",
    "def similarity(claim, wiki):     \n",
    "    claim_wiki = [claim,wiki]\n",
    "    vectorizer = CountVectorizer(claim_wiki)\n",
    "    try:\n",
    "        vectorizer.fit(claim_wiki)\n",
    "        vectors = [vector for vector in vectorizer.transform(claim_wiki).toarray()]\n",
    "        similarity = cosine_similarity(vectors)[1][0]\n",
    "    except:\n",
    "        similarity = 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_list(document):\n",
    "    doc = nlp_large(document)\n",
    "    combine_entity = set()\n",
    "    for entity in doc.ents:\n",
    "        combine_entity.add(str(entity).lower().strip())\n",
    "    return list(combine_entity)\n",
    "\n",
    "def capital(text):\n",
    "    result = re.findall(r\"[A-Z][\\S]+\", text)\n",
    "    result = [w.lower() for w in result]\n",
    "    return result\n",
    "\n",
    "def entity_capital(text):\n",
    "    text = replace_punctuation(text)\n",
    "    entities = get_entity_list(text)\n",
    "    capitals = capital(text)\n",
    "    entities.extend(capitals)\n",
    "    entities = set(entities)\n",
    "    entities = remove_stopwords(entities)\n",
    "    return entities\n",
    "\n",
    "def document_entities(result):\n",
    "    docid = result['found_doc']\n",
    "    pattern = re.compile(r\"(\"+docid+\"\\s\\d+\\s)\")\n",
    "    doc = result['text'].decode(\"utf-8\")\n",
    "    doc = replace_punctuation(doc)\n",
    "    doc = re.sub(pattern,\"\",doc)\n",
    "    doc_entities = entity_capital(doc)\n",
    "    return doc_entities\n",
    "\n",
    "def intersection(list1, list2): \n",
    "    list3 = [value for value in list1 if value in list2] \n",
    "    return len(list3) \n",
    "\n",
    "def similarity_list(claim, wiki):     \n",
    "    claim_wiki = [' '.join(claim).strip(),' '.join(wiki).strip()]\n",
    "    vectorizer = CountVectorizer(claim_wiki)\n",
    "    try:\n",
    "        vectorizer.fit(claim_wiki)\n",
    "        vectors = [vector for vector in vectorizer.transform(claim_wiki).toarray()]\n",
    "        similarity = cosine_similarity(vectors)[1][0]\n",
    "    except:\n",
    "        similarity = 0\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### GET FEATURE LIST ###\n"
     ]
    }
   ],
   "source": [
    "print(\"### GET FEATURE LIST ###\")\n",
    "      \n",
    "      \n",
    "feature_list_path2 = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"features_fulldoc.json\")\n",
    "with open(feature_list_path) as json_file:  \n",
    "    feature_list_dict2 = json.load(json_file)\n",
    "    \n",
    "feature_list2 = []\n",
    "claim_index2 = []\n",
    "for key in feature_list_dict2:\n",
    "    feature_list2.append(feature_list_dict2[key])\n",
    "    claim_index2.append(int(key))\n",
    "    \n",
    "label_list2 = []\n",
    "for i in claim_index2:\n",
    "    label_list2.append(label[i])\n",
    "    \n",
    "weight_path2 = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"weight_fulldoc.json\")\n",
    "with open(weight_path2) as json_file:  \n",
    "    weight_dict2 = json.load(json_file)\n",
    "    \n",
    "weight_list2 = []\n",
    "for i in claim_index2:\n",
    "    weight_list2.append(weight_dict2[str(i)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RANDOM LIST ###\n",
      "### RUN VECTORIZER ###\n"
     ]
    }
   ],
   "source": [
    " print(\"### RANDOM LIST ###\")\n",
    "          \n",
    "    \n",
    "import random\n",
    "\n",
    "combined2 = list(zip(claim_index2, feature_list2, label_list2, weight_list2))\n",
    "random.shuffle(combined2)\n",
    "claim_index2, feature_list2, label_list2, weight_list2 = zip(*combined2)\n",
    "\n",
    "training_set2 = feature_list2[:120000]\n",
    "training_label2 = label_list2[:120000]\n",
    "heldout_set2 = feature_list2[120000:]\n",
    "heldout_label2 = label_list2[120000:]\n",
    "\n",
    "print(\"### RUN VECTORIZER ###\")\n",
    "\n",
    "vectorizer2 = DictVectorizer()\n",
    "training_data2 = vectorizer2.fit_transform(training_set2)\n",
    "heldout_data2 = vectorizer2.transform(heldout_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP Classifier 1 hidden layers 100 nodes ###\n",
    "\n",
    "clf_100 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "clf_100.fit(training_data2, training_label2) \n",
    "\n",
    "predictions_100 = clf_100.predict(heldout_data2)\n",
    "\n",
    "accuracy_100 = accuracy_score(heldout_label2,predictions_100)\n",
    "results_100 = classification_report(heldout_label2,predictions_100,  output_dict=True)\n",
    "\n",
    "print(accuracy_100)\n",
    "print(results_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Document and Sentence Selection </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_vector = en_vectors_web_lg.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dev = defaultdict(Counter)\n",
    "evidence_dev = []\n",
    "evidence_weight = []\n",
    "features_sentence = []\n",
    "for i in range(len(dclaim)):\n",
    "#for i in range(0,20):    \n",
    "    norm_query = replace_punctuation(dclaim[i].lower())  \n",
    "    claim_entities = entity_capital(dclaim[i])\n",
    "    claim_vector = nlp_vector(norm_query)\n",
    "#     query = norm_query + ' ' + ' '.join(claim_entities).strip()\n",
    "#     query = ' '.join(remove_stopwords(query.split())).strip()\n",
    "    query_results, doc_title  = get_match2(norm_query,50)\n",
    "\n",
    "    #print(doc_title)\n",
    "    docid_chosen = []\n",
    "    \n",
    "    # get from IR first #\n",
    "    for ir in range(0,2):\n",
    "        docid_chosen.append(doc_title[ir])\n",
    "        \n",
    "    len_ir = len(docid_chosen)\n",
    "    \n",
    "    #print(docid_chosen)    \n",
    "    \n",
    "    # get from title and entities #\n",
    "    \n",
    "    title_similarity = []\n",
    "    doc_title_title = []\n",
    "    entity_similarity = []\n",
    "    doc_title_entity = []\n",
    "    for j in range(len(doc_title)):\n",
    "        norm_doc_title = doc_title[j].replace(\"_\",\" \")\n",
    "        title_entities = entity_capital(norm_doc_title)\n",
    "        cos_sim_entity = similarity_list(claim_entities,title_entities) \n",
    "        cos_sim_token = similarity(norm_query,norm_doc_title.lower()) \n",
    "        if cos_sim_token > 0:\n",
    "            title_similarity.append(cos_sim_token)\n",
    "            doc_title_title.append(doc_title[j])\n",
    "        if cos_sim_entity > 0:\n",
    "            entity_similarity.append(cos_sim_entity)\n",
    "            doc_title_entity.append(doc_title[j])\n",
    "        \n",
    "        #print(doc_title[j], title_entities, cos_sim_entity, cos_sim_token) \n",
    "    \n",
    "    if len(entity_similarity) > 0:\n",
    "        \n",
    "        combined_entity = list(zip(entity_similarity,doc_title_entity))\n",
    "        \n",
    "        # rerank based on entities and capital words\n",
    "    \n",
    "        combined_entity = sorted(combined_entity, key=lambda x: x[0], reverse=True)   \n",
    "#         print(\"\")       \n",
    "#         print(combined_entity)\n",
    "        limit = min(len(entity_similarity),2)\n",
    "        entity_filtered = combined_entity[:limit]\n",
    "        entity_similarity, docid_entity = list(zip(*entity_filtered))   \n",
    "\n",
    "        for ent in range(0,limit):\n",
    "            if docid_entity[ent] not in docid_chosen:\n",
    "                docid_chosen.append(docid_entity[ent])\n",
    "\n",
    "        len_entity = len(docid_chosen)\n",
    "        \n",
    "    if len(title_similarity) > 0:    \n",
    "        combined_title = list(zip(title_similarity,doc_title_title))\n",
    "\n",
    "        # rerank based on title tokens\n",
    "\n",
    "        combined_title = sorted(combined_title, key=lambda x: x[0], reverse=True)   \n",
    "        \n",
    "#         print(\"\")      \n",
    "#         print(\"sorted title\")\n",
    "#         print(combined_title)\n",
    "        limit = min(len(title_similarity),2)\n",
    "        title_filtered = combined_title[:limit]\n",
    "        title_similarity, docid_title = list(zip(*title_filtered))\n",
    "\n",
    "        #print(docid_title)\n",
    "\n",
    "        for tl in range(0,limit):\n",
    "            if docid_title[tl] not in docid_chosen:\n",
    "                docid_chosen.append(docid_title[tl])   \n",
    "\n",
    "        len_title = len(docid_chosen)\n",
    "    \n",
    "    results = []\n",
    "#     print(dclaim[i],devidence[i])\n",
    "#     print(devidence[i])\n",
    "#     print(docid_chosen)\n",
    "    \n",
    "    for m in range(len(query_results)):\n",
    "        if query_results[m]['found_doc'] in docid_chosen:             \n",
    "            results.append(query_results[m])       \n",
    "    #print(results)\n",
    "    evidence_list = []\n",
    "    sim_score = []\n",
    "    sentence_list = []\n",
    "    weight_list = []\n",
    "    for k in range(len(results)):\n",
    "        doc_id = results[k]['found_doc']\n",
    "        weight = results[k]['weight']        \n",
    "        text = results[k]['text'].decode(\"utf-8\").lower()           \n",
    "        textsplit = text.split('\\\\n')  \n",
    "#         print(len(textsplit))\n",
    "#         print(textsplit)\n",
    "        if len(textsplit) > 0:\n",
    "            for l in range(len(textsplit)-1):\n",
    "                norm_split = replace_punctuation(textsplit[l]).strip()  \n",
    "                sentence_vector = nlp_vector(norm_split)\n",
    "                confidence = claim_vector.similarity(sentence_vector)\n",
    "                sent_id = textsplit[l].split(maxsplit=2)[1]         \n",
    "                evidence_doc_sent = [doc_id,int(sent_id)]\n",
    "                evidence_list.append(evidence_doc_sent)\n",
    "                sim_score.append(confidence)\n",
    "                sentence_list.append(norm_split)  \n",
    "                weight_list.append(weight)\n",
    "                            \n",
    "                    \n",
    "#     print(len(evidence_list))\n",
    "#     print(\"len sim score\", len(sim_score))\n",
    "    \n",
    "    combine_sent = list(zip(sim_score, evidence_list, sentence_list, weight_list))\n",
    "#    print(combine_sent)\n",
    "    combine_sent = sorted(combine_sent, key=lambda x: x[0], reverse=True)\n",
    "    filtered = list(filter(lambda elems: elems[0] >= 0.9, combine_sent))\n",
    "    limit = min(len(filtered),5)\n",
    "    if limit != 0:\n",
    "        source = \"threshold simscore 0.9\"\n",
    "        filtered = filtered[:limit]\n",
    "        sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered))\n",
    "    else:\n",
    "        filtered2 = list(filter(lambda elems: elems[0] >= 0.8, combine_sent))\n",
    "        limit = min(len(filtered2),5)\n",
    "        if limit != 0:\n",
    "            source = \"threshold simscore 0.8\"\n",
    "            filtered2 = filtered2[:limit]\n",
    "            sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered2))\n",
    "        else:\n",
    "            filtered3 = list(filter(lambda elems: elems[0] >= 0.7, combine_sent))\n",
    "            limit = min(len(filtered3),5)\n",
    "            if limit != 0:\n",
    "                source = \"threshold simscore 0.7\"\n",
    "                filtered3 = filtered3[:limit]\n",
    "                sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered3))\n",
    "            else:\n",
    "                filtered4 = list(filter(lambda elems: elems[0] >= 0.6, combine_sent))\n",
    "                limit = min(len(filtered4),5)\n",
    "                if limit != 0:\n",
    "                    source = \"threshold simscore 0.6\"\n",
    "                    filtered4 = filtered4[:limit]\n",
    "                    sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered4))\n",
    "                else:\n",
    "                    source = \"highest simscore\"\n",
    "                    combine_sent = combine_sent[:5]\n",
    "                    sim_score, evidence_list, sentence_list, weight_list = list(zip(*combine_sent))\n",
    "\n",
    "#     print(evidence_list)\n",
    "    for idx in range(len(sentence_list)):\n",
    "#         print(sim_score[idx], evidence_list[idx])\n",
    "       \n",
    "        counter_norm = normalize_counter(sentence_list[idx])  \n",
    "        features_dev[i].update(counter_norm)   \n",
    "        \n",
    "    counter_claim = normalize_counter(norm_query) \n",
    "    features_dev[i].update(counter_claim)\n",
    "    evidence_dev.append(evidence_list)\n",
    "    evidence_weight.append(list(set(weight_list)))\n",
    "    print(i, \"doc:\",len(results), \"IR:\",len_ir, \"-\",source, \"-\",\"entity:\", len_entity, \"title:\",len_title, \"evidence:\",len(evidence_list))    \n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "5001\n",
      "5001\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "dclaim_index = list(features_dev.keys())\n",
    "dlabel_list = []\n",
    "for i in dclaim_index:\n",
    "    dlabel_list.append(dlabel[i])\n",
    "    \n",
    "print(len(features_dev))\n",
    "print(len(dclaim_index))\n",
    "print(len(dlabel_list))\n",
    "print(len(evidence_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_dev = []\n",
    "for key in features_dev:\n",
    "    feature_list_dev.append(features_dev[key])\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "training_data = vectorizer.fit_transform(training_set)\n",
    "dev_data = vectorizer.transform(feature_list_dev)\n",
    "\n",
    "training_matrix_tfidf = transformer.fit_transform(training_data)\n",
    "dev_matrix_tfidf = transformer.fit_transform(dev_data)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indah/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1278: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "training_data = vectorizer.fit_transform(training_set)\n",
    "dev_data = vectorizer.transform(feature_list_dev)\n",
    "\n",
    "training_matrix_tfidf = transformer.fit_transform(training_data)\n",
    "dev_matrix_tfidf = transformer.fit_transform(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35692861427714456\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.33      0.67      0.44      1667\n",
      "        REFUTES       0.45      0.19      0.27      1667\n",
      "       SUPPORTS       0.39      0.21      0.27      1667\n",
      "\n",
      "       accuracy                           0.36      5001\n",
      "      macro avg       0.39      0.36      0.33      5001\n",
      "   weighted avg       0.39      0.36      0.33      5001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf3 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "clf3.fit(training_data, training_label) \n",
    "\n",
    "predictions = clf3.predict(dev_matrix_tfidf)\n",
    "#predictions\n",
    "print(accuracy_score(dlabel_list,predictions))\n",
    "print(classification_report(dlabel_list,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_predictions = {}\n",
    "for i in range(len(drowid)):\n",
    "    dev_set_predictions[drowid[i]] = {\"claim\": dclaim[i], \"label\":predictions[i], \"evidence\":evidence_dev[i]}\n",
    "    \n",
    "with open('dev_set_predictions17.json', 'w') as f:\n",
    "    json.dump(dev_set_predictions, f, indent = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/indah/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:1278: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3639272145570886\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.34      0.67      0.45      1667\n",
      "        REFUTES       0.42      0.18      0.25      1667\n",
      "       SUPPORTS       0.42      0.24      0.31      1667\n",
      "\n",
      "       accuracy                           0.36      5001\n",
      "      macro avg       0.39      0.36      0.33      5001\n",
      "   weighted avg       0.39      0.36      0.33      5001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dev_data2 = vectorizer2.transform(feature_list_dev)\n",
    "\n",
    "training_matrix_tfidf2 = transformer.fit_transform(training_data2)\n",
    "dev_matrix_tfidf2 = transformer.fit_transform(dev_data2)\n",
    "\n",
    "\n",
    "clf_100 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,), random_state=1)\n",
    "\n",
    "clf_100.fit(training_matrix_tfidf2, training_label2) \n",
    "\n",
    "predictions2 = clf_100.predict(dev_matrix_tfidf2)\n",
    "#predictions\n",
    "print(accuracy_score(dlabel_list,predictions2))\n",
    "print(classification_report(dlabel_list,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_predictions2 = {}\n",
    "for i in range(len(drowid)):\n",
    "    dev_set_predictions2[drowid[i]] = {\"claim\": dclaim[i], \"label\":predictions2[i], \"evidence\":evidence_dev[i]}\n",
    "    \n",
    "with open('dev_set_predictions18.json', 'w') as f:\n",
    "    json.dump(dev_set_predictions2, f, indent = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36112777444511096\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.33      0.57      0.42      1667\n",
      "        REFUTES       0.40      0.23      0.29      1667\n",
      "       SUPPORTS       0.40      0.28      0.33      1667\n",
      "\n",
      "       accuracy                           0.36      5001\n",
      "      macro avg       0.38      0.36      0.35      5001\n",
      "   weighted avg       0.38      0.36      0.35      5001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_200 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(200,), random_state=1)\n",
    "\n",
    "clf_200.fit(training_matrix_tfidf2, training_label2) \n",
    "\n",
    "predictions3 = clf_200.predict(dev_matrix_tfidf2)\n",
    "#predictions\n",
    "print(accuracy_score(dlabel_list,predictions3))\n",
    "print(classification_report(dlabel_list,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3667266546690662\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "NOT ENOUGH INFO       0.34      0.63      0.44      1667\n",
      "        REFUTES       0.42      0.22      0.29      1667\n",
      "       SUPPORTS       0.41      0.24      0.31      1667\n",
      "\n",
      "       accuracy                           0.37      5001\n",
      "      macro avg       0.39      0.37      0.35      5001\n",
      "   weighted avg       0.39      0.37      0.35      5001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_300 = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(300,), random_state=1)\n",
    "\n",
    "clf_300.fit(training_matrix_tfidf2, training_label2) \n",
    "\n",
    "predictions4 = clf_300.predict(dev_matrix_tfidf2)\n",
    "#predictions\n",
    "print(accuracy_score(dlabel_list,predictions4))\n",
    "print(classification_report(dlabel_list,predictions4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_predictions4 = {}\n",
    "for i in range(len(drowid)):\n",
    "    dev_set_predictions4[drowid[i]] = {\"claim\": dclaim[i], \"label\":predictions4[i], \"evidence\":evidence_dev[i]}\n",
    "    \n",
    "with open('dev_set_predictions20.json', 'w') as f:\n",
    "    json.dump(dev_set_predictions4, f, indent = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MLPClassifier_lbfgs_300.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(clf_300, 'MLPClassifier_lbfgs_300.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dev = defaultdict(Counter)\n",
    "evidence_dev = []\n",
    "evidence_weight = []\n",
    "features_sentence = []\n",
    "for i in range(len(dclaim)):\n",
    "#for i in range(0,20):    \n",
    "    norm_query = replace_punctuation(dclaim[i].lower())  \n",
    "    claim_entities = entity_capital(dclaim[i])\n",
    "    claim_vector = nlp_vector(norm_query)\n",
    "#     query = norm_query + ' ' + ' '.join(claim_entities).strip()\n",
    "#     query = ' '.join(remove_stopwords(query.split())).strip()\n",
    "    query_results, doc_title  = get_match2(norm_query,50)\n",
    "\n",
    "    #print(doc_title)\n",
    "    docid_chosen = []\n",
    "    \n",
    "    # get from IR first #\n",
    "    for ir in range(0,2):\n",
    "        docid_chosen.append(doc_title[ir])\n",
    "        \n",
    "    len_ir = len(docid_chosen)\n",
    "    \n",
    "    #print(docid_chosen)    \n",
    "    \n",
    "    # get from title and entities #\n",
    "    \n",
    "    title_similarity = []\n",
    "    doc_title_title = []\n",
    "    entity_similarity = []\n",
    "    doc_title_entity = []\n",
    "    for j in range(len(doc_title)):\n",
    "        norm_doc_title = doc_title[j].replace(\"_\",\" \")\n",
    "        title_entities = entity_capital(norm_doc_title)\n",
    "        cos_sim_entity = similarity_list(claim_entities,title_entities) \n",
    "        cos_sim_token = similarity(norm_query,norm_doc_title.lower()) \n",
    "        if cos_sim_token > 0:\n",
    "            title_similarity.append(cos_sim_token)\n",
    "            doc_title_title.append(doc_title[j])\n",
    "        if cos_sim_entity > 0:\n",
    "            entity_similarity.append(cos_sim_entity)\n",
    "            doc_title_entity.append(doc_title[j])\n",
    "        \n",
    "        #print(doc_title[j], title_entities, cos_sim_entity, cos_sim_token) \n",
    "    \n",
    "    if len(entity_similarity) > 0:\n",
    "        \n",
    "        combined_entity = list(zip(entity_similarity,doc_title_entity))\n",
    "        \n",
    "        # rerank based on entities and capital words\n",
    "    \n",
    "        combined_entity = sorted(combined_entity, key=lambda x: x[0], reverse=True)   \n",
    "#         print(\"\")       \n",
    "#         print(combined_entity)\n",
    "        limit = min(len(entity_similarity),2)\n",
    "        entity_filtered = combined_entity[:limit]\n",
    "        entity_similarity, docid_entity = list(zip(*entity_filtered))   \n",
    "\n",
    "        for ent in range(0,limit):\n",
    "            if docid_entity[ent] not in docid_chosen:\n",
    "                docid_chosen.append(docid_entity[ent])\n",
    "\n",
    "        len_entity = len(docid_chosen)\n",
    "        \n",
    "    if len(title_similarity) > 0:    \n",
    "        combined_title = list(zip(title_similarity,doc_title_title))\n",
    "\n",
    "        # rerank based on title tokens\n",
    "\n",
    "        combined_title = sorted(combined_title, key=lambda x: x[0], reverse=True)   \n",
    "        \n",
    "#         print(\"\")      \n",
    "#         print(\"sorted title\")\n",
    "#         print(combined_title)\n",
    "        limit = min(len(title_similarity),2)\n",
    "        title_filtered = combined_title[:limit]\n",
    "        title_similarity, docid_title = list(zip(*title_filtered))\n",
    "\n",
    "        #print(docid_title)\n",
    "\n",
    "        for tl in range(0,limit):\n",
    "            if docid_title[tl] not in docid_chosen:\n",
    "                docid_chosen.append(docid_title[tl])   \n",
    "\n",
    "        len_title = len(docid_chosen)\n",
    "    \n",
    "    results = []\n",
    "#     print(dclaim[i],devidence[i])\n",
    "#     print(devidence[i])\n",
    "#     print(docid_chosen)\n",
    "    \n",
    "    for m in range(len(query_results)):\n",
    "        if query_results[m]['found_doc'] in docid_chosen:             \n",
    "            results.append(query_results[m])       \n",
    "    #print(results)\n",
    "    evidence_list = []\n",
    "    sim_score = []\n",
    "    sentence_list = []\n",
    "    weight_list = []\n",
    "    for k in range(len(results)):\n",
    "        doc_id = results[k]['found_doc']\n",
    "        weight = results[k]['weight']        \n",
    "        text = results[k]['text'].decode(\"utf-8\").lower()           \n",
    "        textsplit = text.split('\\\\n')  \n",
    "#         print(len(textsplit))\n",
    "#         print(textsplit)\n",
    "        if len(textsplit) > 0:\n",
    "            for l in range(len(textsplit)-1):\n",
    "                norm_split = replace_punctuation(textsplit[l]).strip()  \n",
    "                sentence_vector = nlp_vector(norm_split)\n",
    "                confidence = claim_vector.similarity(sentence_vector)\n",
    "                sent_id = textsplit[l].split(maxsplit=2)[1]         \n",
    "                evidence_doc_sent = [doc_id,int(sent_id)]\n",
    "                evidence_list.append(evidence_doc_sent)\n",
    "                sim_score.append(confidence)\n",
    "                sentence_list.append(norm_split)  \n",
    "                weight_list.append(weight)\n",
    "                            \n",
    "                    \n",
    "#     print(len(evidence_list))\n",
    "#     print(\"len sim score\", len(sim_score))\n",
    "    \n",
    "    combine_sent = list(zip(sim_score, evidence_list, sentence_list, weight_list))\n",
    "#    print(combine_sent)\n",
    "    combine_sent = sorted(combine_sent, key=lambda x: x[0], reverse=True)\n",
    "    filtered = list(filter(lambda elems: elems[0] >= 0.95, combine_sent))\n",
    "    limit = min(len(filtered),5)\n",
    "    if limit != 0:\n",
    "        source = \"threshold simscore 0.95\"\n",
    "        filtered = filtered[:limit]\n",
    "        sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered))\n",
    "    else:\n",
    "        filtered2 = list(filter(lambda elems: elems[0] >= 0.9, combine_sent))\n",
    "        limit = min(len(filtered2),5)\n",
    "        if limit != 0:\n",
    "            source = \"threshold simscore 0.9\"\n",
    "            filtered2 = filtered2[:limit]\n",
    "            sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered2))\n",
    "        else:\n",
    "            filtered3 = list(filter(lambda elems: elems[0] >= 0.85, combine_sent))\n",
    "            limit = min(len(filtered3),5)\n",
    "            if limit != 0:\n",
    "                source = \"threshold simscore 0.85\"\n",
    "                filtered3 = filtered3[:limit]\n",
    "                sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered3))\n",
    "            else:\n",
    "                filtered4 = list(filter(lambda elems: elems[0] >= 0.8, combine_sent))\n",
    "                limit = min(len(filtered4),5)\n",
    "                if limit != 0:\n",
    "                    source = \"threshold simscore 0.8\"\n",
    "                    filtered4 = filtered4[:limit]\n",
    "                    sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered4))\n",
    "                else:\n",
    "                    filtered5 = list(filter(lambda elems: elems[0] >= 0.75, combine_sent))\n",
    "                    limit = min(len(filtered5),5)\n",
    "                    if limit != 0:\n",
    "                        source = \"threshold simscore 0.75\"\n",
    "                        filtered5 = filtered5[:limit]\n",
    "                        sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered5))\n",
    "                    else:\n",
    "                        filtered6 = list(filter(lambda elems: elems[0] >= 0.7, combine_sent))\n",
    "                        limit = min(len(filtered6),5)\n",
    "                        if limit != 0:\n",
    "                            source = \"threshold simscore 0.7\"\n",
    "                            filtered6 = filtered6[:limit]\n",
    "                            sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered6))\n",
    "                        else:\n",
    "                            filtered7 = list(filter(lambda elems: elems[0] >= 0.6, combine_sent))\n",
    "                            limit = min(len(filtered7),5)\n",
    "                            if limit != 0:\n",
    "                                source = \"threshold simscore 0.6\"\n",
    "                                filtered7 = filtered7[:limit]\n",
    "                                sim_score, evidence_list, sentence_list, weight_list = list(zip(*filtered7))\n",
    "\n",
    "                            else:\n",
    "                                source = \"highest simscore\"\n",
    "                                combine_sent = combine_sent[:5]\n",
    "                                sim_score, evidence_list, sentence_list, weight_list = list(zip(*combine_sent))\n",
    "\n",
    "#     print(evidence_list)\n",
    "    for idx in range(len(sentence_list)):\n",
    "#         print(sim_score[idx], evidence_list[idx])\n",
    "       \n",
    "        counter_norm = normalize_counter(sentence_list[idx])  \n",
    "        features_dev[i].update(counter_norm)   \n",
    "        \n",
    "    counter_claim = normalize_counter(norm_query) \n",
    "    features_dev[i].update(counter_claim)\n",
    "    evidence_dev.append(evidence_list)\n",
    "    evidence_weight.append(list(set(weight_list)))\n",
    "    print(i, \"doc:\",len(results), \"IR:\",len_ir, \"-\",source, \"-\",\"entity:\", len_entity, \"title:\",len_title, \"evidence:\",len(evidence_list))    \n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "5001\n",
      "5001\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "dclaim_index = list(features_dev.keys())\n",
    "dlabel_list = []\n",
    "for i in dclaim_index:\n",
    "    dlabel_list.append(dlabel[i])\n",
    "    \n",
    "print(len(features_dev))\n",
    "print(len(dclaim_index))\n",
    "print(len(dlabel_list))\n",
    "print(len(evidence_dev))\n",
    "\n",
    "dev_data2 = vectorizer2.transform(feature_list_dev)\n",
    "dev_matrix_tfidf2 = transformer.fit_transform(dev_data2)\n",
    "\n",
    "predictions5 = clf_300.predict(dev_matrix_tfidf2)\n",
    "#predictions\n",
    "print(accuracy_score(dlabel_list,predictions5))\n",
    "print(classification_report(dlabel_list,predictions5))\n",
    "\n",
    "dev_set_predictions21 = {}\n",
    "for i in range(len(drowid)):\n",
    "    dev_set_predictions21[drowid[i]] = {\"claim\": dclaim[i], \"label\":predictions5[i], \"evidence\":evidence_dev[i]}\n",
    "with open('dev_set_predictions21.json', 'w') as f:\n",
    "    json.dump(dev_set_predictions21, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
